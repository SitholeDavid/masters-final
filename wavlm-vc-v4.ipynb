{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T05:43:44.839098Z","iopub.execute_input":"2023-10-02T05:43:44.839457Z","iopub.status.idle":"2023-10-02T05:43:55.823410Z","shell.execute_reply.started":"2023-10-02T05:43:44.839428Z","shell.execute_reply":"2023-10-02T05:43:55.822155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd \nimport numpy as np\nimport shutil\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nfrom scipy.io import wavfile\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n\nimport torchaudio\n\nfrom einops import repeat\nfrom einops.layers.torch import Rearrange\n\n# downloading pre-trained models\nimport urllib.request\n\n# trimming silences in .wav files\nfrom pydub import AudioSegment\n\nwandb_api_key = '430e8c7ef92cf79a3d7c3d02e3d961257153181f'\nos.environ[\"WANDB_API_KEY\"] = wandb_api_key","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:43:55.825760Z","iopub.execute_input":"2023-10-02T05:43:55.826136Z","iopub.status.idle":"2023-10-02T05:44:10.465038Z","shell.execute_reply.started":"2023-10-02T05:43:55.826090Z","shell.execute_reply":"2023-10-02T05:44:10.464090Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:10.466406Z","iopub.execute_input":"2023-10-02T05:44:10.466815Z","iopub.status.idle":"2023-10-02T05:44:10.499001Z","shell.execute_reply.started":"2023-10-02T05:44:10.466771Z","shell.execute_reply":"2023-10-02T05:44:10.497353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.save('/kaggle/working/start-token.npy',torch.randn(1024).numpy())\nSTART_TOKEN = torch.from_numpy(np.load('/kaggle/working/start-token.npy')).unsqueeze(0)\n\n#np.save('/kaggle/working/stop-token.npy',torch.randn(1024).numpy())\nSTOP_TOKEN = torch.from_numpy(np.load('/kaggle/working/stop-token.npy')).unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:10.502092Z","iopub.execute_input":"2023-10-02T05:44:10.502880Z","iopub.status.idle":"2023-10-02T05:44:10.528082Z","shell.execute_reply.started":"2023-10-02T05:44:10.502842Z","shell.execute_reply":"2023-10-02T05:44:10.526895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source_content_path = '/kaggle/input/europarl-source-content-features'\ntarget_content_path = '/kaggle/input/europarl-target-content-features'\ntarget_speaker_path = '/kaggle/input/europarl-target-speaker-features'","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:10.529434Z","iopub.execute_input":"2023-10-02T05:44:10.530411Z","iopub.status.idle":"2023-10-02T05:44:10.535444Z","shell.execute_reply.started":"2023-10-02T05:44:10.530378Z","shell.execute_reply":"2023-10-02T05:44:10.534047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"two_channel_wavs = np.load('/kaggle/input/europarl-extra-files/two_channel_wavs.npy')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:10.536773Z","iopub.execute_input":"2023-10-02T05:44:10.537988Z","iopub.status.idle":"2023-10-02T05:44:10.555326Z","shell.execute_reply.started":"2023-10-02T05:44:10.537953Z","shell.execute_reply":"2023-10-02T05:44:10.554147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_SEQUENCE_LENGTH = 350 + 1 # 1 tokens for SOS \n\nclass CLVCDataset(Dataset):\n    def __init__(self, source_content_path, target_content_path, target_speaker_path, max_sequence_length=270):\n        self.max_sequence_length = max_sequence_length\n        \n        self.source_content_embeddings = glob(f'{source_content_path}/*.content.npy')\n        self.target_content_embeddings = glob(f'{target_content_path}/*.content.npy')\n        self.target_speaker_embeddings = glob(f'{target_speaker_path}/*.se.npy')\n        \n        self.source_content_embeddings = sorted(list(filter(lambda x: x.split('/')[-1].replace('.content.npy', '') not in two_channel_wavs, self.source_content_embeddings)))\n        self.target_content_embeddings = sorted(list(filter(lambda x: x.split('/')[-1].replace('.content.npy', '') not in two_channel_wavs, self.target_content_embeddings)))\n        self.target_speaker_embeddings = sorted(list(filter(lambda x: x.split('/')[-1].replace('.se.npy', '') not in two_channel_wavs, self.target_speaker_embeddings)))\n        \n        self._trim_dataset()\n        \n        assert len(self.target_content_embeddings) != 0, 'Length of content embeddings may not be zero.'\n        assert len(self.target_content_embeddings) == len(self.target_speaker_embeddings), 'Target speaker content embeddings must be same length as target speaker embeddings'\n        assert len(self.target_content_embeddings) == len(self.source_content_embeddings), 'Source content embeddings must be same length as target content embeddings'\n        \n    def __len__(self):\n        return len(self.source_content_embeddings)\n    \n    def _trim_dataset(self):\n        files_to_remove = np.load('/kaggle/input/europarl-extra-files/files_to_remove.npy')            \n        self.source_content_embeddings = sorted(list(filter(lambda x: x.split('/')[-1].replace('.content.npy', '') not in files_to_remove, self.source_content_embeddings)))\n        self.target_content_embeddings = sorted(list(filter(lambda x: x.split('/')[-1].replace('.content.npy', '') not in files_to_remove, self.target_content_embeddings)))\n        self.target_speaker_embeddings = sorted(list(filter(lambda x: x.split('/')[-1].replace('.se.npy', '') not in files_to_remove, self.target_speaker_embeddings)))\n        #self.files_to_remove = files_to_remove\n\n    @staticmethod\n    def pad_sequence(sequence, max_sequence_length):\n        seq_len = sequence.shape[1]\n        pad_len = max(0, max_sequence_length - seq_len)\n        \n        if pad_len > 0:\n            sequence = F.pad(sequence, (0, 0, 0, pad_len, 0, 0))\n            \n        return sequence\n    \n    def __getitem__(self, idx):\n        # load pre-computed embeddings\n        \n        source_content_embed = np.load(self.source_content_embeddings[idx])[:MAX_SEQUENCE_LENGTH, :]\n        target_content_embed = np.load(self.target_content_embeddings[idx])[:MAX_SEQUENCE_LENGTH - 1, :] # subtract 1 to accomodate start token\n        target_speaker_embed = np.load(self.target_speaker_embeddings[idx])\n       \n        # numpy array -> torch tensor\n        target_content_embed = torch.from_numpy(target_content_embed).unsqueeze(0)\n        target_speaker_embed = torch.from_numpy(target_speaker_embed)\n        source_content_embed = torch.from_numpy(source_content_embed).unsqueeze(0)\n        \n        optim_target_content_embed = torch.concat((target_content_embed, STOP_TOKEN.unsqueeze(0)), dim=1)   \n        target_content_embed = torch.concat((START_TOKEN.unsqueeze(0), target_content_embed), dim=1)\n        \n        # get lengths\n        source_length = torch.tensor(source_content_embed.shape[1]) \n        target_length = torch.tensor(target_content_embed.shape[1])  \n       \n        # pad content sequences\n        source_content_embed = self.pad_sequence(source_content_embed, self.max_sequence_length).squeeze(0)\n        target_content_embed = self.pad_sequence(target_content_embed, self.max_sequence_length).squeeze(0)\n        optim_target_content_embed = self.pad_sequence(optim_target_content_embed, self.max_sequence_length).squeeze(0)\n    \n        return source_content_embed, target_content_embed, optim_target_content_embed, target_speaker_embed, source_length, target_length","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:10.556901Z","iopub.execute_input":"2023-10-02T05:44:10.557468Z","iopub.status.idle":"2023-10-02T05:44:10.573212Z","shell.execute_reply.started":"2023-10-02T05:44:10.557436Z","shell.execute_reply":"2023-10-02T05:44:10.572091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CLVCDataset(source_content_path=source_content_path,\n                      target_content_path=target_content_path,\n                      target_speaker_path=target_speaker_path,\n                      max_sequence_length=MAX_SEQUENCE_LENGTH)\n\ntrain_split = int(len(dataset) * 0.95)\n\ntrain_dataset, val_dataset =  torch.utils.data.random_split(dataset, [train_split, len(dataset) - train_split])\ntrain_loader = DataLoader(train_dataset, batch_size=48, shuffle=True, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=48, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:10.574684Z","iopub.execute_input":"2023-10-02T05:44:10.575318Z","iopub.status.idle":"2023-10-02T05:44:12.829785Z","shell.execute_reply.started":"2023-10-02T05:44:10.575286Z","shell.execute_reply":"2023-10-02T05:44:12.828825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modules","metadata":{}},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len):\n        \"\"\"\n        Inputs\n            d_model - Hidden dimensionality of the input.\n            max_len - Maximum length of a sequence to expect.\n        \"\"\"\n        super().__init__()\n\n        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n\n        # register_buffer => Tensor which is not a parameter, but should be part of the modules state.\n        # Used for tensors that need to be on the same device as the module.\n        # persistent=False tells PyTorch to not add the buffer to the state dict (e.g. when we save the model)\n        self.register_buffer('pe', pe, persistent=False)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:12.831176Z","iopub.execute_input":"2023-10-02T05:44:12.832172Z","iopub.status.idle":"2023-10-02T05:44:12.840986Z","shell.execute_reply.started":"2023-10-02T05:44:12.832136Z","shell.execute_reply":"2023-10-02T05:44:12.840033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerDecoderBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout, expansion_factor=2):\n        super().__init__()\n        \n        self.target_norm = nn.LayerNorm(embed_dim)\n        self.query_norm = nn.LayerNorm(embed_dim)\n        self.memory_norm = nn.LayerNorm(embed_dim)\n        \n        self.self_attention = nn.MultiheadAttention(embed_dim, num_heads, dropout, batch_first=True)\n        self.sa_dropout = nn.Dropout(dropout)\n        \n        self.mlp = nn.Sequential(\n            nn.LayerNorm(embed_dim),\n            Rearrange('B S E -> B E S'),\n            nn.Conv1d(embed_dim, int(embed_dim * expansion_factor), 9, padding = (9 - 1) // 2),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Conv1d(int(embed_dim * expansion_factor), embed_dim, 1, padding = (1 - 1) // 2),\n            nn.Dropout(dropout),\n            Rearrange('B E S-> B S E')\n        )\n        \n    def forward(self,\n                query,\n                memory,\n                query_padding_mask=None,\n                query_fill_mask=None,\n                memory_padding_mask=None,\n                memory_fill_mask=None\n               ): \n        \n        if query_fill_mask is not None:\n            query.masked_fill_(query_fill_mask, 0.0)\n            \n        query_in = self.query_norm(query)\n        memory_in = self.memory_norm(memory)\n        \n        sa_attn_out, _ = self.self_attention(query_in,\n                                             memory_in,\n                                             memory_in,\n                                             key_padding_mask=memory_padding_mask)\n        \n        if query_fill_mask is not None:\n            sa_attn_out.masked_fill_(query_fill_mask, 0.0)\n            \n        x = self.sa_dropout(sa_attn_out) + query\n        x = self.mlp(self.target_norm(x)) + x\n        \n        if query_fill_mask is not None:\n            x.masked_fill_(query_fill_mask, 0.0)\n        \n        return x       ","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:12.845163Z","iopub.execute_input":"2023-10-02T05:44:12.845443Z","iopub.status.idle":"2023-10-02T05:44:12.856287Z","shell.execute_reply.started":"2023-10-02T05:44:12.845421Z","shell.execute_reply":"2023-10-02T05:44:12.855141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WavLMDecoder(nn.Module):\n    def __init__(self, \n                 speaker_embedding_dim,\n                 content_embedding_dim,\n                 hidden_dim,\n                 num_heads,\n                 num_layers,\n                 max_sequence_length,\n                 expansion_factor=2,\n                 dropout=0.0,\n                 duration_dropout=0.0\n                ):\n        super().__init__()\n        \n        self.content_embedding_dim = content_embedding_dim \n        self.max_sequence_length = max_sequence_length\n        self.hidden_dim = hidden_dim \n        \n        self.positional_embedding = PositionalEncoding(hidden_dim, self.max_sequence_length)\n        self.speaker_embedding = nn.Linear(speaker_embedding_dim, hidden_dim)\n        self.content_embedding = nn.Linear(content_embedding_dim, hidden_dim)\n        self.norm = nn.LayerNorm(hidden_dim)\n        \n        self.stop_token_predictor = nn.Sequential(\n            nn.LayerNorm(self.content_embedding_dim),\n            Rearrange('B S E -> B E S'),\n            nn.Conv1d(self.content_embedding_dim, self.content_embedding_dim, 9, padding = (9 - 1) // 2),\n            nn.GELU(),\n            nn.Dropout(duration_dropout),\n            nn.Conv1d(self.content_embedding_dim, 1, 1, padding = (1 - 1) // 2),\n            Rearrange('B 1 S-> B S')\n        )\n        \n        self.layers = nn.Sequential(*[TransformerDecoderBlock(hidden_dim,\n                                                              num_heads,\n                                                              dropout,\n                                                              expansion_factor)  for _ in range(num_layers)])\n        \n        self.hidden_2_wavlm = nn.Linear(self.hidden_dim, self.content_embedding_dim)\n        \n        \n    def forward(self, source_content, source_lengths, reference_embedding):\n        src_padding_masks, src_fill_masks = self.get_masks(source_lengths, self.max_sequence_length, self.hidden_dim)\n         \n        ref_embedding = self.speaker_embedding(reference_embedding)\n        ref_embedding = repeat(ref_embedding, 'B 1 E -> B S E', S=self.max_sequence_length)\n        \n        x = self.positional_embedding(self.content_embedding(source_content)) + ref_embedding \n        memory = self.positional_embedding(self.content_embedding(source_content)) + ref_embedding \n        \n        for idx, layer in enumerate(self.layers):\n            x = layer(query=x,\n                      memory=memory,\n                      query_padding_mask= src_padding_masks if idx < 1 else None,\n                      query_fill_mask= src_fill_masks if idx < 1 else None,\n                      memory_padding_mask= src_padding_masks,\n                      memory_fill_mask= src_fill_masks\n                     )\n        \n        x = self.hidden_2_wavlm(x)\n        stop_token_out = self.stop_token_predictor(x)\n        return x, stop_token_out\n    \n    @staticmethod\n    def get_masks(seq_lens, max_seq_len, embed_dim):\n        B = seq_lens.shape[0]\n        masks = [[ mask_idx >= seq_lens[seq_idx]  for mask_idx in torch.arange(max_seq_len)]  for seq_idx in torch.arange(0, B)]\n        masks = torch.tensor(masks, dtype=torch.bool)\n        fill_masks = repeat(masks.T, 'b s -> e b s', e=embed_dim).T.contiguous()\n        \n        return masks.to(device), fill_masks.to(device)\n    \n    @staticmethod\n    def get_causal_masks(max_seq_len):\n        return ~torch.tril(torch.ones((max_seq_len,max_seq_len), dtype=torch.bool, device=device))\n    \n# dec = WavLMDecoder(speaker_embedding_dim=512, content_embedding_dim=1024, hidden_dim=256, num_heads=2, num_layers=3, max_sequence_length=351)\n# dec(source_content = torch.randn(3, 351, 1024), source_lengths = torch.tensor([100, 150, 200]), reference_embedding = torch.randn(3, 1, 512))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:12.857899Z","iopub.execute_input":"2023-10-02T05:44:12.858265Z","iopub.status.idle":"2023-10-02T05:44:12.875858Z","shell.execute_reply.started":"2023-10-02T05:44:12.858233Z","shell.execute_reply":"2023-10-02T05:44:12.874787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WavLMVCLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.feature_loss = nn.L1Loss()\n        self.stop_token_loss = nn.CrossEntropyLoss()\n        \n    def forward(self,\n                feature_predictions,\n                feature_targets,\n                stop_token_predictions,\n                stop_token_targets,\n                feature_masks):\n        \n        feature_masks = ~feature_masks\n        feature_masks.requires_grad = False\n        feature_targets.requires_grad = False\n        stop_token_targets.requires_grad = False\n        \n        batch_size = feature_predictions.shape[0]\n        feature_masks = repeat(feature_masks, 'b s -> b s e', e=feature_predictions.shape[-1])\n        masked_feature_predictions = feature_predictions.masked_select(feature_masks).float()\n        masked_feature_targets = feature_targets.masked_select(feature_masks).float()\n        \n        feature_loss = self.feature_loss(masked_feature_predictions, masked_feature_targets)\n        stop_token_loss = self.stop_token_loss(stop_token_predictions, stop_token_targets)\n        total_loss = feature_loss + stop_token_loss\n        \n        return (total_loss, feature_loss, stop_token_loss)   ","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:12.877365Z","iopub.execute_input":"2023-10-02T05:44:12.878318Z","iopub.status.idle":"2023-10-02T05:44:12.890624Z","shell.execute_reply.started":"2023-10-02T05:44:12.878285Z","shell.execute_reply":"2023-10-02T05:44:12.889992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class WavLMVC(pl.LightningModule):\n    def __init__(self, **kwargs):\n        super().__init__()\n        self.save_hyperparameters()\n        self.model = WavLMDecoder(**kwargs)\n        self.loss_module = WavLMVCLoss()\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), betas=(0.9, 0.98))\n        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50,100], gamma=0.1)\n        return [optimizer], [lr_scheduler]\n    \n    def training_step(self, batch, batch_idx):\n        (source_content_embed,\n         target_content_embed,\n         optim_target_content_embed,\n         target_speaker_embed,\n         source_length,\n         target_length) = batch\n        \n        (target_predictions, stop_token_predictions) = self.model(source_content=source_content_embed,\n                                                                  source_lengths=source_length,   \n                                                                  reference_embedding=target_speaker_embed)\n        \n        trg_padding_masks, _ = WavLMDecoder.get_masks(target_length,\n                                                      self.hparams.max_sequence_length,\n                                                      self.hparams.hidden_dim)\n        \n        (total_loss, feature_loss, stop_token_loss) = self.loss_module(feature_predictions=target_predictions,\n                                                                       feature_targets=optim_target_content_embed,\n                                                                       stop_token_predictions=stop_token_predictions,\n                                                                       stop_token_targets=target_length,\n                                                                       feature_masks=trg_padding_masks)\n        \n        self.log(f'train_total_loss', total_loss, on_step=True, on_epoch=True)\n        self.log(f'train_feature_loss', feature_loss, on_step=True, on_epoch=True)\n        self.log(f'train_stop_loss', stop_token_loss, on_step=True, on_epoch=True)\n        \n        return total_loss\n    \n    def validation_step(self, batch, batch_idx):\n        (source_content_embed,\n         target_content_embed,\n         optim_target_content_embed,\n         target_speaker_embed,\n         source_length,\n         target_length) = batch\n        \n        (target_predictions, stop_token_predictions) = self.model(source_content=source_content_embed,\n                                                                  source_lengths=source_length,   \n                                                                  reference_embedding=target_speaker_embed)\n        \n        trg_padding_masks, _ = WavLMDecoder.get_masks(target_length,\n                                                      self.hparams.max_sequence_length,\n                                                      self.hparams.hidden_dim)\n        \n        (total_loss, feature_loss, stop_token_loss) = self.loss_module(feature_predictions=target_predictions,\n                                                                       feature_targets=optim_target_content_embed,\n                                                                       stop_token_predictions=stop_token_predictions,\n                                                                       stop_token_targets=target_length,\n                                                                       feature_masks=trg_padding_masks)\n        \n        self.log(f'val_total_loss', total_loss, on_step=True, on_epoch=True)\n        self.log(f'val_feature_loss', feature_loss, on_step=True, on_epoch=True)\n        self.log(f'val_stop_loss', stop_token_loss, on_step=True, on_epoch=True)\n    \n    def test_step(self, batch, batch_idx):\n        (source_content_embed,\n         target_content_embed,\n         optim_target_content_embed,\n         target_speaker_embed,\n         source_length,\n         target_length) = batch\n        \n        (target_predictions, stop_token_predictions) = self.model(source_content=source_content_embed,\n                                                                  source_lengths=source_length,   \n                                                                  reference_embedding=target_speaker_embed)\n        \n        trg_padding_masks, _ = WavLMDecoder.get_masks(target_length,\n                                                      self.hparams.max_sequence_length,\n                                                      self.hparams.hidden_dim)\n        \n        (total_loss, feature_loss, stop_token_loss) = self.loss_module(feature_predictions=target_predictions,\n                                                                       feature_targets=optim_target_content_embed,\n                                                                       stop_token_predictions=stop_token_predictions,\n                                                                       stop_token_targets=target_length,\n                                                                       feature_masks=trg_padding_masks)\n        \n        self.log(f'test_total_loss', total_loss, on_step=True, on_epoch=True)\n        self.log(f'test_feature_loss', feature_loss, on_step=True, on_epoch=True)\n        self.log(f'test_stop_loss', stop_token_loss, on_step=True, on_epoch=True)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:12.892286Z","iopub.execute_input":"2023-10-02T05:44:12.892939Z","iopub.status.idle":"2023-10-02T05:44:12.909759Z","shell.execute_reply.started":"2023-10-02T05:44:12.892899Z","shell.execute_reply":"2023-10-02T05:44:12.908696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train the model","metadata":{}},{"cell_type":"code","source":"import wandb\nwandb.finish()\nproject = 'WavLMVC'\nmodel_version = 'v:2.0.2'\nwandb_logger = WandbLogger(project=project, name=model_version)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:12.911433Z","iopub.execute_input":"2023-10-02T05:44:12.912413Z","iopub.status.idle":"2023-10-02T05:44:47.151505Z","shell.execute_reply.started":"2023-10-02T05:44:12.912373Z","shell.execute_reply":"2023-10-02T05:44:47.150551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(**kwargs):\n    trainer = pl.Trainer(default_root_dir=os.path.join('/kaggle/working/checkpoints', f'WavLM VC - {model_version}'),\n                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n                         devices=1,\n                         max_epochs=None,\n                         max_steps=5000,\n                         logger=wandb_logger,\n                         gradient_clip_val=1.0,\n                         check_val_every_n_epoch=1,\n                         overfit_batches=4,\n                         num_sanity_val_steps=0,\n                         limit_val_batches=4,\n#                          callbacks=[\n#                              ModelCheckpoint(save_weights_only=False,\n#                                              save_last=True,\n#                                              mode=\"min\", monitor=\"val_total_loss\",\n#                                              save_top_k=3\n#                                             ),\n                                             \n#                             ModelCheckpoint(save_weights_only=False,\n#                                      save_last=True,\n#                                      mode=\"max\", monitor=\"step\",\n#                                      every_n_train_steps =500,\n#                                      save_top_k=3\n#                                     )]\n                        )\n    \n    model = WavLMVC(**kwargs)\n    num_params = sum([p.numel() for p in model.parameters()])\n    print(f'Number of params: {num_params}')\n    \n    trainer.fit(model, train_loader, val_loader)\n    model = WavLMVC.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n    return model, trainer","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:47.155825Z","iopub.execute_input":"2023-10-02T05:44:47.158031Z","iopub.status.idle":"2023-10-02T05:44:47.170207Z","shell.execute_reply.started":"2023-10-02T05:44:47.157996Z","shell.execute_reply":"2023-10-02T05:44:47.168926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, results = train_model(\n    speaker_embedding_dim=512,\n    content_embedding_dim=1024,\n    hidden_dim=512,\n    num_heads=2,\n    num_layers=4,\n    max_sequence_length=MAX_SEQUENCE_LENGTH,\n    expansion_factor=2,\n    dropout=0.0,\n    duration_dropout=0.0)  ","metadata":{"execution":{"iopub.status.busy":"2023-10-02T05:44:47.172104Z","iopub.execute_input":"2023-10-02T05:44:47.172983Z","iopub.status.idle":"2023-10-02T06:00:07.100865Z","shell.execute_reply.started":"2023-10-02T05:44:47.172949Z","shell.execute_reply":"2023-10-02T06:00:07.099574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}